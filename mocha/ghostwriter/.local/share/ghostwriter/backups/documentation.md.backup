---
title: "*rockscale* — An Algorithmic Video Upscaler with ROCm"
subtitle: "**SIT315 Task M4.T1D: Project Documentation**\\vspace{2em}"
author: "Keo Ponleou Sok"
date: "\\today"
titlepage: true
titlepage-rule-height: 1.5

output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    cover: true
    number_sections: true
    keep_tex: true

geometry: a4paper

header-includes:
  - \usepackage[dvipsnames]{xcolor}
  - \usepackage{graphicx}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{listings}
  - \usepackage{fancyhdr}
  - \usepackage[en-AU]{datetime2}
  
  - \setcounter{secnumdepth}{3}
  
  - \lstset{
      basicstyle=\ttfamily\small,
      breaklines=true,
      columns=flexible,
      showstringspaces=false,
      keywordstyle=\color{blue}\bfseries,
      commentstyle=\color{teal},
      stringstyle=\color{brown},
      identifierstyle=\color{black}
    } 
    
  - \AtBeginDocument{\hypersetup{linkcolor=black,urlcolor=black,citecolor=black}}
  
  - \usepackage{titling} 
  - \renewcommand\maketitlehooka{\null\vfill\thispagestyle{empty}}
  - \renewcommand\maketitlehookd{\vfill\null}
  
  - \usepackage{tocloft}
  - \renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
  - \renewcommand{\cftsubsecleader}{\cftdotfill{\cftdotsep}}
  - \renewcommand{\cftsubsubsecleader}{\cftdotfill{\cftdotsep}}
  
  - \fancypagestyle{plain}{\fancyhf{}\renewcommand{\headrulewidth}{0pt}} 
...

<div style="page-break-after: always;">\pagebreak</div>

\tableofcontents

<div style="page-break-after: always;">\pagebreak</div>

# Introduction

*rockscale* is a CLI program built with HIP to optimise GPU performance to upscale video using interpolation algorithms. The program uses FFmpeg to decode videos into individual frames, and each frames are copied into the device memory, where HIP interpolation kernels are ran to interpolate pixels within the frame. The program offers 3 different interpolation algorithms—bilinear, bicubic and lanczos. After interpolation, each frames are copied back to host memory, and are passed into the encoder to output a new, upscaled video. Both software and hardware encoder options are provided. Tested video configurations include `mp4`, `mkv`, and `mov` containers, with `h264` and `h265/hevc` codec. Other video file configurations may work, but have not been tested. Though HIP theoretically should be able to compile the program to utilise CUDA platform, only ROCm has been tested due to accessibility. Additionally, only `vaapi` hardware encoders have been tested.

# Architecture

The architecture of the program is a multithreaded producer-processor-consumer model, with each role capable of running in multiple threads to share workload. The producer in this program is the decoder, decoding the video packets into individual frames, in which thread workers are organised by the FFmpeg library itself. Frame are then passed into a bounded queue, in which the processor can consume in order to upscale through interpolation.

The processors are responsible for taking frames from the decoder's queue, in which the frames are then processed through interpolation. This interpolation processing uses HIP kernel to take advantage of GPU parallel computing. Each interpolation algorithm—bilinear, bicubic, and lanczos—have its own series of kernel functions in order to process the frames. When multiple threads are assigned to the processor role, each thread will consume its own frame from the decoder buffer, and pass kernel functions into its own kernel streams, allowing ROCm (or the parallel computing platform) to optimise multiple streams processing, and ensuring optimal device utilisation. Each processing threads then pass the upscaled frame into the encoder's bounded priority queue.

The consumer runs FFmpeg's encoder, which takes frames from its priority queue, and packages them into a new file's streams, and closes the file once queue is empty. Additionally, if a hardware encoder is chosen for the consumer, the processors have an extra step to copy the frame data into a hardware frame in the device's memory before appending it in the encoder's priority queue. The encoder's priority queue works differently from a traditional FIFO queue. Since encoding frames into packages requires ordering the frames by its `pts` value, all frames passed into the priority queue are ordered by ascending `pts` and the encoder consumes each frame from the buffer in that order.

## Architectural Limitations

Due to the requirements of a bounded encoder buffer, with strict priority of frames' `pts` order during encoding, it poses a limitation when used with multiple processing threads. Since the decoder produces frames also within the ascending `pts` order, a processor thread pick frames from the decoder queue in that order as well, and ideally, append the processed frame in the encoder's queue before picking up the next frame. If this was the case, then a priority queue for the encoder queue would not be necessary as it is guaranteed that the order of frames is determined by the decoder's output. The problem arises when multiple processor threads will each take its own frames, process them, and append them to the encoder queue in a non-deterministic order. In this case, it is possible, and rather common, that the frames inside the encoder queue is not in the same order as the output from the decoder.

Hence, a priority queue is required to accommodate the fact that the frames inputted in the queue is not in the correct order. It should be realised that a priority queue would only function when the frames are picked when there is more than one frame in the queue, so that the queue will compare the `pts` of the different frames inside the queue and output the smallest `pts` available in the queue. In the ideal scenario the encoder's queue should be unbounded, and frames from the queue should be consumed only when all frames are appended. Only this way would we guarantee that the frames consumed by the encoder is in perfect `pts` order. Unfortunately, this is not feasible for 2 reasons. For one, this would mean that the producer-processor cannot run in parallel with the consumer, as it would need to wait for all frames to be processed before the consumer can encode the frames. For two, an unbounded queue will reach host memory limitations with ease, especially in the context of each individual upscaled video frames. The size of all frames will scale by the scaling factor of the upscale, and the length of the video. Therefore, the most optimal approach is a bounded priority queue, with a threshold that determines the minimum frames in the queue required before allowing to consume a frame. This will allow frames to accumulate to reach the threshold, then consuming a frame from the priority queue will return the smallest `pts` frame available in the queue.

Despite using a threshold, it still does not guarantee that the frames output from the priority buffer will be the expected frame `pts` when multiple processor threads are running. In a situation where a specific processor thread consumes and is processing a frame, however, other threads finish multiple frames (enough to reach the queue threshold) before that specific thread finishes its one frame, that frame's `pts` would still be stuck under processing while the encoder has already consumed multiple frames. If that frame's `pts` is the next ordered frame expected by the encoder, but still hasn't reached the queue, this would caused a miss ordered frame encoding. In the case when the encoder receives an unexpected `pts` frame, the encoder simply rejects the frame, causing artifacts in the video. However, the program minimises this issue by overwriting the frame's `pts` with the expected value, but the incorrect frame positioning in the video is still a problem in the output.

This issue is simply a limitation of a bounded queue that requires specific ordering of data produced by a non-deterministic order of multiple producers. Of course, this problem can be eliminated by simply having one processor thread, so that the order is fully deterministic, while maintaining the producer-processor-consumer model. The downside is potentially slower processing performance, therefore, it is still useful to keep the implementation of multiple processor threads. Through experimentation, the queue size and threshold value that is "reliable-enough" for minimal misplaced frames, and optimal memory consumption was $$\text{queue size} = \text{processor threads} \times 4$$ $$\text{threshold} = 0.75 \times \text{queue size}$$.

# Interpolation Algorithms

This program offers 3 different interpolation algorithm for upscaling, that is, bilinear, bicubic, and lanczos interpolations. All 3 algorithms are implemented with a series of HIP kernels, utilising global, shared memory, and memory buffer reusage to optimise performance.

Each individual video frames to be interpolated are in YUV pixel format, where each frame would have 3 different data planes, the Y plane, U plane, and V plane. The Y plane, which contains the brightness data, is a flatten 2-dimension array, where its shape reflects the video's resolution. The chroma planes, which are U and V planes, are also a flatten 2-dimension array, however, its shape is scaled down from the original video's resolution depending on the chroma subsampling. Unlike pixel formats such as RGB, or BGR, a frame will contain 3 separate planes as most formats do, however, the chroma planes (U and V planes) tend to be smaller with chroma subsampling. This allows YUV pixel format to be the most efficient for image/video processing, requiring less data to be processed while containing the same information.

Hence, this program ensures that all frames inside the decode buffer should be converted to a YUV pixel format by the processor before interpolating them in the kernels. Each series of kernel runs for a particular interpolation will occur 3 times to process a single frame, one time for each plane in the frame.

## Bilinear

Bilinear is the most simple interpolation out of the three. Its principle is to simply assume that the values between two pixels changes linearly. Hence, for $n$ scaling, $n-1$ new pixel data is created for each pixel, where the values follow between the linear gradient of the current pixel data to the next. Since the data plane to be interpolated is 2-dimensional, the linear interpolation is calculated twice, one across the columns, and one more across the rows, hence, the name bilinear.

The program implements this bilinear interpolation using two HIP kernels. The first kernel interpolates the linear data across each columns for all columns. For the current column, new pixel data are interpolated with the next columns' data, all the way until the last column of the plane. However, note that the last column does not have a next column to calculate the linear gradient against. Hence, this program implementation accommodates this by stretching the previous gradient across, in assumption that the data after the last column follows the same linear gradient as the previous gradient.

Once the columns are interpolated, the same interpolation is calculated across each row, including the new data from the generated columns. Similarly, the last row in the plane also stretches the previous linear gradient in assumption that the gradient continues from the previous 2 points.

For the current pixel, $x_t$, and the next pixel, $x_{t+1}$, and a scaling of $n$, the gradient is calculated by $$g=\frac{x_{t+1} - x_t}{n}$$ whereby the $n-1$ new pixels is interpolated through $$x_i = x_t + (i \cdot g),\quad i\in\{1,\dots, n-1\}$$

Additionally, the pixels within the last column/row, $x_k$, interpolates $n-1$ pixels after $x_k$ using $$g=\frac{x_{k} - x_{k-1}}{n}$$ $$x_j = x_k + ((j+n) \cdot g),\quad j\in\{1,\dots, n-1\}$$

Furthermore, each interpolated data value are clamped between value $0$ and $255$ inclusive, as the planes are a flatten array of bytes, with a maximum value of 8-bits.

## Bicubic

The bicubic interpolation follows the same approach as the bilinear interpolation, except the values between two pixels constructs a cubic spline rather than a linear line following a gradient. The interpolation for new pixels is first completed across the columns, and then repeated across each row, where the interpolated values are calculated by the cubic spline between the two pixels. Similarly, data is interpolated after the pixel in the last column or row by stretching the cubic spline from the previous two pixels.

For the current pixel, $x_t$, and the next pixel, $x_{t+1}$, the cubic spline is calculated with $$f(i) = \{[(2x_t + m_t - 2x_{t+1} + m_{t+1})i + (-3x_t+3x_{t+1}-2m_t-m_{t+1})]i + m_t\}i + x_t$$

## Lanczos

## Memory 

# The Video Pipeline and decisions 

(about hardware encoder, software encoder being the bottleneck, or maybe this can be introduced in performance tests)

# Performance Tests

# Results comparison (algorithm compare)